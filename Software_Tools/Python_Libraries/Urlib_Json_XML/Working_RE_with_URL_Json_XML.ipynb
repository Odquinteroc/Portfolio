{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Working With Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "x = open('text2.txt')\n",
    "h =[]\n",
    "for line in x:\n",
    "    line.rstrip()\n",
    "    y = re.findall('[0-9]+', line)\n",
    "    if len(y) < 1 :continue\n",
    "    for i in y:\n",
    "        h.append(int(i))\n",
    "m = sum(h)\n",
    "print(m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using BeautifulSoup and urllib to parse a HTML website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ') #https://py4e-data.dr-chuck.net/comments_42.html , https://py4e-data.dr-chuck.net/comments_1965933.html\n",
    "html = urlopen(url, context=ctx).read() # <class 'bytes'>\n",
    "#print('html: ', type(html))\n",
    "soup = BeautifulSoup(html, \"html.parser\") # <class 'bs4.BeautifulSoup'>\n",
    "#print('soup: ', type(soup)) # <class 'bs4.BeautifulSoup'>\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('span') # <class 'bs4.element.ResultSet'>\n",
    "#print('tags: ', type(tags))\n",
    "suma = 0\n",
    "for tag in tags:\n",
    "    #print(type(tag))#<class 'bs4.element.Tag'>\n",
    "    #print(tag.contents) # <class 'list'>\n",
    "    suma += int(tag.contents[0])\n",
    "    print('Contents:', tag.contents[0])\n",
    "print(suma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retriving: https://py4e-data.dr-chuck.net/known_by_Fikret.html\n",
      "retriving: http://py4e-data.dr-chuck.net/known_by_Owain.html\n",
      "retriving: http://py4e-data.dr-chuck.net/known_by_Filip.html\n",
      "retriving: http://py4e-data.dr-chuck.net/known_by_Jorryn.html\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter URL: ') # https://py4e-data.dr-chuck.net/known_by_Fikret.html , https://py4e-data.dr-chuck.net/known_by_J.html\n",
    "a = int(input('Enter count: '))\n",
    "Line = int(input('Enter position: '))\n",
    "\n",
    "print('retriving:', url)\n",
    "\n",
    "cn= 0\n",
    "while cn < a:\n",
    "    cn +=1\n",
    "    html = urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    all_a_tags = soup.find_all('a')\n",
    "    third_a_tag = all_a_tags[Line - 1]\n",
    "    print('retriving:', str(third_a_tag.get('href', None)))\n",
    "    url = str(third_a_tag.get('href', None))\n",
    "\n",
    "# for i in range(0, a):\n",
    "#     html = urlopen(url, context=ctx).read()\n",
    "#     soup = BeautifulSoup(html, \"html.parser\")\n",
    "#     tags = soup('a')\n",
    "#     count = 0\n",
    "\n",
    "#     for tag in tags:\n",
    "#         count+=1\n",
    "#         if count == Line:\n",
    "#             print('retriving:', str(tag.get('href', None)))\n",
    "#             url = str(tag.get('href', None))\n",
    "#             count = 0\n",
    "#             break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using xml.etree.ElementTree and urllib to parse a XML website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2291\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import urllib.parse, urllib.error\n",
    "import urllib.request \n",
    "\n",
    "url = 'https://py4e-data.dr-chuck.net/comments_42.xml'\n",
    "u = 'https://py4e-data.dr-chuck.net/comments_1965935.xml'\n",
    "response = urllib.request.urlopen(u) #type class http.client.HTTPResponse'>\n",
    "#print('response',type(response),response)\n",
    "xml_data = response.read() #type bytes, \n",
    "#print('xml_data',type(xml_data))\n",
    "\n",
    "stuff = ET.fromstring(xml_data) # class 'xml.etree.ElementTree.Element\n",
    "#print('stuff',type(stuff))\n",
    "lst = stuff.findall('comments/comment') #list of bytes\n",
    "#print(lst)\n",
    "suma = 0\n",
    "lista =[]\n",
    "for i in lst:\n",
    "    a = i.find('count').text #this fount the tag count and convertit from bytes to strings\n",
    "    #print(a)\n",
    "    lista.append(int(a))\n",
    "c = sum(lista)\n",
    "\n",
    "print(c)\n",
    "    \n",
    "# for item in lst:\n",
    "#      a = item.find('count').text\n",
    "#      b = int(a)\n",
    "#      suma += b\n",
    "# print(suma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using JSon and urllib to parse a Json website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Romina', 'count': 97}, {'name': 'Laurie', 'count': 97}, {'name': 'Siyona', 'count': 90}, {'name': 'Bayli', 'count': 90}, {'name': 'Taisha', 'count': 88}, {'name': 'Alanda', 'count': 87}, {'name': 'Ameelia', 'count': 87}, {'name': 'Prasheeta', 'count': 80}, {'name': 'Risa', 'count': 79}, {'name': 'Asif', 'count': 79}, {'name': 'Zi', 'count': 78}, {'name': 'Ediomi', 'count': 76}, {'name': 'Danyil', 'count': 76}, {'name': 'Lance', 'count': 72}, {'name': 'Barry', 'count': 72}, {'name': 'Mathu', 'count': 66}, {'name': 'Hattie', 'count': 66}, {'name': 'Bowie', 'count': 65}, {'name': 'Samara', 'count': 65}, {'name': 'Uchenna', 'count': 64}, {'name': 'Georgia', 'count': 61}, {'name': 'Shauni', 'count': 61}, {'name': 'Rivan', 'count': 59}, {'name': 'Kenan', 'count': 58}, {'name': 'Isma', 'count': 57}, {'name': 'Hassan', 'count': 57}, {'name': 'Samanthalee', 'count': 54}, {'name': 'Alexa', 'count': 51}, {'name': 'Caine', 'count': 49}, {'name': 'Grady', 'count': 47}, {'name': 'Anne', 'count': 40}, {'name': 'Rihan', 'count': 38}, {'name': 'Alexei', 'count': 37}, {'name': 'Indie', 'count': 36}, {'name': 'Rhuairidh', 'count': 36}, {'name': 'Annoushka', 'count': 32}, {'name': 'Kenzi', 'count': 25}, {'name': 'Shahd', 'count': 24}, {'name': 'Irvine', 'count': 22}, {'name': 'Carys', 'count': 21}, {'name': 'Skye', 'count': 19}, {'name': 'Rohan', 'count': 18}, {'name': 'Atiya', 'count': 18}, {'name': 'Nuala', 'count': 14}, {'name': 'Maram', 'count': 12}, {'name': 'Carlo', 'count': 12}, {'name': 'Japleen', 'count': 9}, {'name': 'Breeanna', 'count': 7}, {'name': 'Zaaine', 'count': 3}, {'name': 'Inika', 'count': 2}]\n",
      "2553\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import urllib.parse, urllib.error\n",
    "import urllib.request \n",
    "import ssl\n",
    "\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "data = input('enter:') # https://py4e-data.dr-chuck.net/comments_42.json ,  https://py4e-data.dr-chuck.net/comments_1965936.json\n",
    "\n",
    "uh = urllib.request.urlopen(data, context=ctx)\n",
    "#print('uh' , type(uh)) # <class 'http.client.HTTPResponse'>\n",
    "url = uh.read().decode()\n",
    "#print('url' , type(url)) # url <class 'str'>\n",
    "\n",
    "info = json.loads(url) #info <class 'dict'>\n",
    "#print('info', type(info)) # info <class 'dict'>\n",
    "\n",
    "c=0\n",
    "#print(\"info['comments']\" , type(info['comments'])) # info['comments'] <class 'list'>\n",
    "# for i in info['comments']:\n",
    "#     c += i['count']\n",
    "\n",
    "inf = info['comments'] #list of dictionaries\n",
    "print(inf)\n",
    "for i in inf:\n",
    "    c += i['count']\n",
    "\n",
    "print(c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q': 'Harvard University'}\n",
      "Retrieving https://py4e-data.dr-chuck.net/opengeo?q=Harvard+University\n",
      "Retrieved 1488 characters {\"type\":\"FeatureColl\n",
      "Retriving: https://www.openstreetmap.org/copyright\n",
      "3\n",
      "plus_code 87JC9V9F+57 url https://www.openstreetmap.org/copyright\n",
      "Harvard University, Revere Street, Cambridge, MA 02163, United States of America\n",
      "{'q': 'harvard university'}\n",
      "Retrieving https://py4e-data.dr-chuck.net/opengeo?q=harvard+university\n",
      "Retrieved 1488 characters {\"type\":\"FeatureColl\n",
      "Retriving: https://www.openstreetmap.org/copyright\n",
      "3\n",
      "plus_code 87JC9V9F+57 url https://www.openstreetmap.org/copyright\n",
      "Harvard University, Revere Street, Cambridge, MA 02163, United States of America\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse\n",
    "import json, ssl\n",
    "\n",
    "# Heavily rate limited proxy of https://www.geoapify.com/ api\n",
    "serviceurl = 'https://py4e-data.dr-chuck.net/opengeo?' # Url with the interrogation symbols at the end to start a query of any specifics location in the api geoapify\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "    address = input('Enter location: ')\n",
    "    if len(address) < 1: break\n",
    "\n",
    "    address = address.strip() #Take the location and remove whitespace\n",
    "    parms = dict()\n",
    "    parms['q'] = address #store the location in a dictionary with 'q' as key and address as value\n",
    "    print(parms)\n",
    "    url = serviceurl + urllib.parse.urlencode(parms) # concatenate the url and the location in orden to create a url with the specific location,  encode the location with .urlencode in order to be queryable through url \n",
    "\n",
    "\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "    data = uh.read().decode()\n",
    "    print('Retrieved', len(data), 'characters', data[:20].replace('\\n', ' '))\n",
    "\n",
    "    try:\n",
    "        js = json.loads(data)\n",
    "    except:\n",
    "        js = None\n",
    "\n",
    "    if not js or 'features' not in js:\n",
    "        print('==== Download error ===')\n",
    "        print(data)\n",
    "        break\n",
    "\n",
    "    if len(js['features']) == 0:\n",
    "        print('==== Object not found ====')\n",
    "        print(data)\n",
    "        break\n",
    "\n",
    "    # print(json.dumps(js, indent=4))\n",
    "\n",
    "    plus_code = js['features'][0]['properties']['plus_code']\n",
    "    url = js['features'][0]['properties']['datasource']['url']\n",
    "    print('Retriving:', url)\n",
    "    print(len(js))\n",
    "    print('plus_code', plus_code, 'url', url)\n",
    "    location = js['features'][0]['properties']['formatted']\n",
    "    print(location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
